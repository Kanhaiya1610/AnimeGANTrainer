{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "import os\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install Kaggle API\n",
    "!pip install kaggle\n",
    "\n",
    "# Create Kaggle directory\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# Create kaggle.json (REPLACE WITH YOUR CREDENTIALS)\n",
    "!echo '{\"username\":\"<your_kaggle_username>\",\"key\":\"<your_31_letter_kaggel_key>\"}' > ~/.kaggle/kaggle.json\n",
    "\n",
    "# Set permissions\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(os.path.expanduser(\"~/.kaggle/kaggle.json\"))\n",
    "\n",
    "# Download Anime Face Dataset\n",
    "!kaggle datasets download -d splcher/animefacedataset -p /content/drive/<your_google_drive_name>/\n",
    "\n",
    "# Unzip dataset\n",
    "!unzip /content/drive/<your_google_drive_name>/animefacedataset.zip -d /content/drive/<your_google_drive_name>/anime_dataset\n",
    "\n",
    "# Used for cleanup if needed, good practice\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the target size required by the ViT model\n",
    "VIT_INPUT_SIZE = 224\n",
    "\n",
    "class AnimeDatasetPreprocessor:\n",
    "    \"\"\"\n",
    "    Prepares an anime image dataset by selecting a sample, resizing images\n",
    "    to the ViT's required input size (224x224), and splitting into\n",
    "    train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------------------------|\n",
    "    #                                                                                                                    |\n",
    "    # Change the sample_Size, Start with Smaller no. like 500 then gradually increase after every successfull training.  |\n",
    "    #                                                                                                                    |\n",
    "    #--------------------------------------------------------------------------------------------------------------------|\n",
    "\n",
    "    def __init__(self, input_dir, output_dir, sample_size=1000, target_size=VIT_INPUT_SIZE):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.sample_size = sample_size\n",
    "        self.target_size = target_size # Store the target size (224, 224)\n",
    "\n",
    "        if not os.path.isdir(input_dir):\n",
    "             raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "        if not os.path.isdir(os.path.join(input_dir, 'images')):\n",
    "             raise FileNotFoundError(f\"Subdirectory 'images' not found in: {input_dir}\")\n",
    "\n",
    "        # Create output directories safely\n",
    "        self.train_dir = os.path.join(output_dir, 'train')\n",
    "        self.test_dir = os.path.join(output_dir, 'test')\n",
    "        self.val_dir = os.path.join(output_dir, 'validation')\n",
    "        os.makedirs(self.train_dir, exist_ok=True)\n",
    "        os.makedirs(self.test_dir, exist_ok=True)\n",
    "        os.makedirs(self.val_dir, exist_ok=True)\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads images, selects a sample, splits, resizes, and saves them.\n",
    "        \"\"\"\n",
    "        images_subdir = os.path.join(self.input_dir, 'images')\n",
    "        try:\n",
    "            image_files = [\n",
    "                f for f in os.listdir(images_subdir)\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "            ]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: 'images' subdirectory not found at {images_subdir}\")\n",
    "            return\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No images found in {images_subdir}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Total images found: {len(image_files)}\")\n",
    "\n",
    "        # Ensure sample size is not larger than available images\n",
    "        actual_sample_size = min(self.sample_size, len(image_files))\n",
    "        if actual_sample_size < self.sample_size:\n",
    "            print(f\"Warning: Requested sample size {self.sample_size} > images available {len(image_files)}. Using {actual_sample_size}.\")\n",
    "\n",
    "        # Randomly select images\n",
    "        selected_images = np.random.choice(\n",
    "            image_files,\n",
    "            size=actual_sample_size,\n",
    "            replace=False\n",
    "        )\n",
    "        print(f\"Selected {len(selected_images)} images for processing.\")\n",
    "\n",
    "        if len(selected_images) < 3: # Need at least 3 images for train/val/test split\n",
    "             print(\"Error: Not enough selected images to create train/validation/test splits.\")\n",
    "             return\n",
    "\n",
    "        # Split dataset (e.g., 70% train, 15% validation, 15% test)\n",
    "        train_images, temp_images = train_test_split(\n",
    "            selected_images,\n",
    "            test_size=0.3, # 30% left for val+test\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Split the remainder into validation and test (50% of temp = 15% of total)\n",
    "        val_images, test_images = train_test_split(\n",
    "            temp_images,\n",
    "            test_size=0.5, # 50% of temp is test\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # --- Preprocessing function ---\n",
    "        def process_and_save_images(image_list, split_folder_path):\n",
    "            count = 0\n",
    "            for img_name in image_list:\n",
    "                input_path = os.path.join(images_subdir, img_name)\n",
    "                output_filename = os.path.splitext(img_name)[0] + \".png\" # Consider saving as PNG\n",
    "                output_path = os.path.join(split_folder_path, output_filename)\n",
    "\n",
    "                try:\n",
    "                    # Open, convert to RGB (important!), resize, and save\n",
    "                    img = Image.open(input_path).convert('RGB')\n",
    "                    # --- KEY CHANGE: Resize to the target size (224x224) ---\n",
    "                    img_resized = img.resize((self.target_size, self.target_size), Image.LANCZOS)\n",
    "                    # --- Save (consider PNG for consistency) ---\n",
    "                    img_resized.save(output_path, format='PNG')\n",
    "                    count += 1\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"Warning: Image file not found during processing: {input_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_name}: {e}\")\n",
    "            return count\n",
    "        # --- End of Preprocessing function ---\n",
    "\n",
    "        # Process and save images for each split\n",
    "        print(f\"\\nProcessing Training images (saving to {self.train_dir})...\")\n",
    "        train_count = process_and_save_images(train_images, self.train_dir)\n",
    "        print(f\"Processed {train_count} training images.\")\n",
    "\n",
    "        print(f\"\\nProcessing Validation images (saving to {self.val_dir})...\")\n",
    "        val_count = process_and_save_images(val_images, self.val_dir)\n",
    "        print(f\"Processed {val_count} validation images.\")\n",
    "\n",
    "        print(f\"\\nProcessing Test images (saving to {self.test_dir})...\")\n",
    "        test_count = process_and_save_images(test_images, self.test_dir)\n",
    "        print(f\"Processed {test_count} test images.\")\n",
    "\n",
    "        # Final dataset stats\n",
    "        print(\"\\n--- Dataset Preparation Summary ---\")\n",
    "        print(f\"Target image size: {self.target_size}x{self.target_size}\")\n",
    "        print(f\"Total images selected: {len(selected_images)}\")\n",
    "        print(f\"Training images saved: {train_count} (in {self.train_dir})\")\n",
    "        print(f\"Validation images saved: {val_count} (in {self.val_dir})\")\n",
    "        print(f\"Testing images saved: {test_count} (in {self.test_dir})\")\n",
    "        print(\"-----------------------------------\\n\")\n",
    "\n",
    "# Example Usage (adjust paths as needed)\n",
    "print(\"Running Preprocessor...\")\n",
    "preprocessor = AnimeDatasetPreprocessor(\n",
    "    input_dir='/content/drive/<your_google_drive_name>/anime_dataset',                # Directory containing the 'images' subfolder\n",
    "    output_dir='/content/drive/<your_google_drive_name>/processed_anime_dataset_224', # New output directory for 224x224 images\n",
    "    sample_size=1000                                                 # Number of images to sample, change to 500 and start again (caution: don't run locally unless you have minimum RTX 4090)\n",
    ")                                                                    # with RAM more than 16GB and C-Drive space(minimum) 1TB. \n",
    "print(os.path.expanduser(\"~/.content/drive/<your_google_drive_name>/processed_anime_dataset_224\"))\n",
    "preprocessor.prepare_dataset()\n",
    "print(\"Preprocessor finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights # Example ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets # Make sure datasets is imported\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                            COMPLETE COLAB SCRIPT\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. IMPORTS\n",
    "# ------------------------------------------\n",
    "import glob\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from sklearn.model_selection import train_test_split # For preprocessor split\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 2. MOUNT GOOGLE DRIVE\n",
    "# ------------------------------------------\n",
    "print(\"Mounting Google Drive...\")\n",
    "try:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "    print(\"Please ensure you have authorized Google Drive access.\")\n",
    "    # Depending on the workflow, you might want to exit here\n",
    "    # raise SystemExit(\"Drive mounting failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 3. CONFIGURATION\n",
    "# ------------------------------------------\n",
    "# --- Paths ---\n",
    "# !! ADJUST THESE PATHS !!\n",
    "DRIVE_SAVE_DIR = '/content/drive/<your_google_drive_name>/Colab_Outputs/AnimeVAE' # Base directory on Drive for saving models/images\n",
    "RAW_DATASET_DIR = '/content/drive/<your_google_drive_name>/anime_dataset/images' # Path on Drive to the folder containing the 'images' subfolder\n",
    "# ------------------\n",
    "PROCESSED_DATA_DIR_NAME = '/drive/<your_google_drive_name>/processed_anime_dataset_224' # Name for the folder where resized images will be stored (locally in Colab runtime)\n",
    "PROCESSED_DATA_ROOT = f'./{PROCESSED_DATA_DIR_NAME}' # Local path for processed data\n",
    "\n",
    "# --- Model & Data Parameters ---\n",
    "VIT_INPUT_SIZE = 224        # Input size required by ViT-B/16\n",
    "LATENT_DIM = 256            # Dimensionality of the VAE latent space\n",
    "BETA = 1.0                  # Weight for the KL divergence term (Beta-VAE)\n",
    "\n",
    "# --- Training Parameters ---\n",
    "LEARNING_RATE = 0.0001      # Learning rate for the Adam optimizer (VAE might need smaller LR)\n",
    "BATCH_SIZE = 32              # Adjust based on GPU memory (start lower if memory errors occur)\n",
    "NUM_EPOCHS = 400             # Number of training epochs (as requested)\n",
    "SAVE_EVERY_EPOCHS = 10       # How often to save the model weights to Drive\n",
    "GENERATE_EVERY_EPOCHS = 10   # How often to generate and save sample images during training\n",
    "# --- Preprocessing Parameters ---\n",
    "SAMPLE_SIZE_PREPROCESSOR = 2000 # Max number of images to process from the raw dataset\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "NUM_IMAGES_TO_GENERATE = 16 # Number of images to generate after training\n",
    "\n",
    "\n",
    "# --- Create Save Directory on Drive ---\n",
    "os.makedirs(DRIVE_SAVE_DIR, exist_ok=True)\n",
    "print(f\"Save directory on Drive: {DRIVE_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 4.2 Custom Dataset Class ---\n",
    "class ImageDatasetNoLabels(Dataset):\n",
    "    \"\"\"Loads images from a single folder (no class subdirectories).\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        patterns = ['*.png', '*.PNG', '*.jpg', '*.JPG', '*.jpeg', '*.JPEG']\n",
    "        self.image_files = []\n",
    "        for pattern in patterns:\n",
    "             self.image_files.extend(glob.glob(os.path.join(root_dir, pattern)))\n",
    "        self.image_files = sorted(self.image_files)\n",
    "\n",
    "        if not self.image_files:\n",
    "            # Raise error if the folder is empty after preprocessing claims success\n",
    "            raise FileNotFoundError(f\"CRITICAL: No image files found in the processed directory {root_dir}. Check preprocessing output.\")\n",
    "        print(f\"Initialized Dataset: Found {len(self.image_files)} images in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading image {img_path}: {e}. Returning dummy tensor.\")\n",
    "            # Return a dummy tensor of the correct size if loading fails\n",
    "            return torch.zeros((3, VIT_INPUT_SIZE, VIT_INPUT_SIZE))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return only the image tensor (DataLoader handles batching)\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_and_save_samples(samples_tensor, epoch_num, save_dir, prefix=\"generated\"):\n",
    "    \"\"\"Denormalizes, visualizes, and saves a grid of generated images.\"\"\"\n",
    "    if samples_tensor is None or samples_tensor.shape[0] == 0:\n",
    "        print(f\"[{prefix} Epoch {epoch_num}] No samples provided for visualization.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[{prefix} Epoch {epoch_num}] Visualizing and saving {samples_tensor.shape[0]} samples...\")\n",
    "    try:\n",
    "        # Ensure samples are on CPU\n",
    "        samples_tensor = samples_tensor.cpu()\n",
    "\n",
    "        # Denormalize images (using ImageNet stats assumed in DataLoader)\n",
    "        inv_normalize = transforms.Normalize(\n",
    "           mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "           std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "        samples_denorm = torch.stack([inv_normalize(img) for img in samples_tensor])\n",
    "        samples_denorm = torch.clamp(samples_denorm, 0, 1) # Ensure valid range [0, 1]\n",
    "\n",
    "        # Create grid\n",
    "        grid = vutils.make_grid(samples_denorm,\n",
    "                                padding=2,\n",
    "                                normalize=False, # Already in [0,1]\n",
    "                                nrow=int(samples_denorm.shape[0]**0.5)) # Make grid square-ish\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(grid.permute(1, 2, 0).numpy()) # CHW -> HWC for matplotlib\n",
    "        plt.title(f'{prefix.capitalize()} Anime VAE Images (Epoch {epoch_num})')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # Saving to Drive\n",
    "        os.makedirs(save_dir, exist_ok=True) # Ensure directory exists\n",
    "        save_image_path = os.path.join(save_dir, f'{prefix}_anime_vae_samples_epoch_{epoch_num}.png')\n",
    "        vutils.save_image(samples_denorm, save_image_path, normalize=False)\n",
    "        print(f\"[{prefix} Epoch {epoch_num}] Image grid saved to: {save_image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during visualization or saving of {prefix} images at epoch {epoch_num}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the target size required by the ViT model\n",
    "VIT_INPUT_SIZE = 224\n",
    "\n",
    "# --- Dummy Classes (Replace with your actual implementations) ---\n",
    "class VisionTransformerVAE(nn.Module):\n",
    "    def __init__(self, latent_dim, vit_input_size):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_size = vit_input_size\n",
    "        # Dummy layers - replace with actual ViT Encoder + Decoder + Latent mapping\n",
    "        self.encoder_dummy = nn.Linear(3 * vit_input_size * vit_input_size, 512)\n",
    "        self.fc_mu = nn.Linear(512, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)\n",
    "        self.decoder_input = nn.Linear(latent_dim, 512)\n",
    "        self.decoder_dummy = nn.Linear(512, 3 * vit_input_size * vit_input_size)\n",
    "        print(f\"Placeholder VisionTransformerVAE initialized (Latent: {latent_dim}, Input: {vit_input_size})\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Flatten input for dummy linear layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        h = F.relu(self.encoder_dummy(x))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.decoder_input(z))\n",
    "        recon = torch.sigmoid(self.decoder_dummy(h)) # Use sigmoid for [0,1] range\n",
    "        # Reshape back to image format\n",
    "        return recon.view(recon.size(0), 3, self.input_size, self.input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "class ImageDatasetNoLabels(torch.utils.data.Dataset):\n",
    "     def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        try:\n",
    "            self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir)\n",
    "                                if os.path.isfile(os.path.join(root_dir, f))\n",
    "                                and f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if not self.image_files:\n",
    "                raise FileNotFoundError(f\"No image files found in {root_dir}\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error initializing Dataset: {e}\")\n",
    "            raise\n",
    "        print(f\"Placeholder ImageDatasetNoLabels found {len(self.image_files)} images in {root_dir}\")\n",
    "\n",
    "     def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "     def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Error loading image {img_path}: {e}. Returning None.\")\n",
    "             # Returning None might cause issues in DataLoader if not handled properly.\n",
    "             # A better approach might be to return a dummy tensor or skip the file.\n",
    "             # For simplicity here, we rely on the dataloader's default collate to potentially raise error later.\n",
    "             # Or, modify your dataloader's collate_fn to filter out Nones.\n",
    "             # Let's return a dummy tensor of the expected size. Needs transform first.\n",
    "             dummy_tensor = torch.zeros((3, VIT_INPUT_SIZE, VIT_INPUT_SIZE)) # Assuming VIT_INPUT_SIZE is accessible\n",
    "             return dummy_tensor # Or handle differently in collate_fn\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                      IMPORTS AND SETUP (Assumed)\n",
    "# ==============================================================================\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AnimeGANTrainer:\n",
    "    \"\"\"Handles VAE training, checkpointing, saving, and generation.\"\"\"\n",
    "\n",
    "    def __init__(self, model, learning_rate, beta, device,\n",
    "                 model_save_path, checkpoint_path, generate_every_epochs,\n",
    "                 start_epoch=0, optimizer_state=None):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.model_save_path = model_save_path # Base path for final .pth file\n",
    "        self.checkpoint_path = checkpoint_path # Path for .pth checkpoint file\n",
    "        self.generate_every_epochs = generate_every_epochs\n",
    "        # Ensure results_save_dir exists even if model_save_path dir doesn't initially\n",
    "        base_save_dir = os.path.dirname(model_save_path)\n",
    "        if not base_save_dir: # Handle case where path is just a filename\n",
    "             base_save_dir = \".\"\n",
    "        self.results_save_dir = os.path.join(base_save_dir, \"training_generations\")\n",
    "        os.makedirs(self.results_save_dir, exist_ok=True) # Create generation dir\n",
    "\n",
    "        self.current_epoch = start_epoch # The last *completed* epoch\n",
    "\n",
    "        # Define Loss (using reduction='sum' as common for VAEs, average later)\n",
    "        self.reconstruction_loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        # Initialize Optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        print(f\"Optimizer initialized: Adam, LR: {self.learning_rate}\")\n",
    "\n",
    "        # Load Optimizer State if resuming from checkpoint\n",
    "        if optimizer_state:\n",
    "            print(\"Loading optimizer state from checkpoint...\")\n",
    "            try:\n",
    "                self.optimizer.load_state_dict(optimizer_state)\n",
    "                # Move optimizer state tensors to the correct device\n",
    "                for state in self.optimizer.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            state[k] = v.to(self.device)\n",
    "                print(\"Optimizer state loaded successfully.\")\n",
    "                print(f\"  Optimizer LR after loading state: {self.optimizer.param_groups[0]['lr']}\") # Log loaded LR\n",
    "            except Exception as e:\n",
    "                print(f\"\\nWarning: Could not load optimizer state: {e}\")\n",
    "                print(\"Optimizer will start from scratch.\")\n",
    "        else:\n",
    "             print(\"No optimizer state provided, starting optimizer fresh.\")\n",
    "\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Trainer Initialized:\")\n",
    "        print(f\"  Device: {self.device}\")\n",
    "        print(f\"  Start Epoch (Next to Run): {self.current_epoch + 1}\") # Training starts at next epoch\n",
    "        print(f\"  Learning Rate: {self.optimizer.param_groups[0]['lr']}\") # Current LR in optimizer\n",
    "        print(f\"  Beta (KL Weight): {self.beta}\")\n",
    "        print(f\"  Checkpoint Path: {self.checkpoint_path}\")\n",
    "        print(f\"  Final Model Save Path: {self.model_save_path}\")\n",
    "        print(f\"  Generate Images Every: {self.generate_every_epochs} epochs\")\n",
    "        print(f\"  Intermediate Images Dir: {self.results_save_dir}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, epoch_completed):\n",
    "        \"\"\"Saves a checkpoint including model, optimizer, epoch, and hyperparams.\"\"\"\n",
    "        if not self.checkpoint_path:\n",
    "            print(\"Warning: Checkpoint path not set. Cannot save checkpoint.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(self.checkpoint_path), exist_ok=True)\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch_completed, # Record the epoch that just finished\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr'], # Save current LR from optimizer\n",
    "            'beta': self.beta\n",
    "        }\n",
    "        try:\n",
    "            # Use a temporary file and rename for atomicity (safer saving)\n",
    "            temp_path = self.checkpoint_path + \".tmp\"\n",
    "            torch.save(checkpoint, temp_path)\n",
    "            os.replace(temp_path, self.checkpoint_path) # Atomic rename\n",
    "            print(f\"Checkpoint saved successfully for epoch {epoch_completed} to {self.checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving checkpoint for epoch {epoch_completed}: {e}\")\n",
    "            if os.path.exists(temp_path):\n",
    "                 os.remove(temp_path) # Clean up temp file on error\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"Saves only the model's state_dict.\"\"\"\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        try:\n",
    "            print(f\"\\nSaving model state_dict to: {path}\")\n",
    "            # Use a temporary file and rename for atomicity\n",
    "            temp_path = path + \".tmp\"\n",
    "            torch.save(self.model.state_dict(), temp_path)\n",
    "            os.replace(temp_path, path)\n",
    "            print(\"Model state_dict saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving model state_dict to {path}: {e}\")\n",
    "            if os.path.exists(temp_path):\n",
    "                os.remove(temp_path)\n",
    "\n",
    "\n",
    "    def calculate_vae_loss(self, reconstructed_x, original_x, mu, logvar):\n",
    "        \"\"\"Calculates the VAE loss components.\"\"\"\n",
    "        # Reconstruction Loss (sum over pixels and channels for each image)\n",
    "        recon_loss = self.reconstruction_loss_fn(reconstructed_x, original_x)\n",
    "\n",
    "        # KL Divergence (sum over latent dimensions for each image)\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1) # Sum over latent dim\n",
    "\n",
    "        # Average losses over the batch\n",
    "        batch_size = original_x.size(0)\n",
    "        if batch_size == 0:\n",
    "             return torch.tensor(0.0, device=self.device), torch.tensor(0.0), torch.tensor(0.0)\n",
    "\n",
    "        avg_recon_loss_per_sample = recon_loss / batch_size\n",
    "        avg_kl_div_per_sample = torch.sum(kl_div) / batch_size\n",
    "\n",
    "        # Total loss per sample\n",
    "        total_loss = avg_recon_loss_per_sample + self.beta * avg_kl_div_per_sample\n",
    "\n",
    "        return total_loss, avg_recon_loss_per_sample, avg_kl_div_per_sample\n",
    "\n",
    "\n",
    "    def train(self, train_loader, epochs_to_run, total_target_epochs, save_every):\n",
    "        \"\"\"Trains the model for a specified number of epochs with checkpointing.\"\"\"\n",
    "        if epochs_to_run <= 0:\n",
    "            print(\"No epochs remaining to run based on start_epoch and total_target_epochs.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n--- Starting Training ---\")\n",
    "        print(f\"Running for {epochs_to_run} epochs (Epoch {self.current_epoch + 1} to {total_target_epochs})\")\n",
    "        start_time_total = time.time()\n",
    "        self.model.train()\n",
    "\n",
    "        initial_epoch_completed = self.current_epoch\n",
    "\n",
    "        for epoch_idx in range(epochs_to_run):\n",
    "            self.current_epoch = initial_epoch_completed + 1 + epoch_idx\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            total_epoch_loss = 0.0\n",
    "            total_epoch_recon_loss = 0.0\n",
    "            total_epoch_kl_loss = 0.0\n",
    "            samples_processed_this_epoch = 0\n",
    "\n",
    "            progress_bar = tqdm(enumerate(train_loader),\n",
    "                                total=len(train_loader),\n",
    "                                desc=f\"Epoch {self.current_epoch}/{total_target_epochs}\",\n",
    "                                unit=\"batch\",\n",
    "                                leave=True)\n",
    "\n",
    "            for i, batch_data in progress_bar:\n",
    "                try:\n",
    "                    images = batch_data.to(self.device, non_blocking=True)\n",
    "                    batch_size = images.size(0)\n",
    "                    if batch_size == 0: continue\n",
    "                except Exception as e:\n",
    "                     print(f\"\\nError moving batch {i} to device in epoch {self.current_epoch}: {e}\")\n",
    "                     continue\n",
    "\n",
    "                if images.ndim != 4 or images.shape[1] != 3:\n",
    "                    print(f\"\\nWarning: Unexpected batch tensor shape {images.shape} in epoch {self.current_epoch}, batch {i}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                try:\n",
    "                    reconstructed_images, mu, logvar = self.model(images)\n",
    "                    loss, recon_loss, kl_div = self.calculate_vae_loss(reconstructed_images, images, mu, logvar)\n",
    "                except Exception as e:\n",
    "                     print(f\"\\nError during forward pass or loss calculation in epoch {self.current_epoch}, batch {i}: {e}\")\n",
    "                     traceback.print_exc()\n",
    "                     print(\"Skipping batch.\")\n",
    "                     continue\n",
    "\n",
    "                if not torch.isfinite(loss):\n",
    "                    print(f\"\\nWarning: NaN or Inf loss detected in epoch {self.current_epoch}, batch {i}. Loss: {loss.item()}. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError during backward pass or optimizer step in epoch {self.current_epoch}, batch {i}: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    print(\"Skipping optimizer step for this batch.\")\n",
    "                    continue\n",
    "\n",
    "                batch_loss_val = loss.item()\n",
    "                batch_recon_loss_val = recon_loss.item()\n",
    "                batch_kl_loss_val = kl_div.item()\n",
    "\n",
    "                total_epoch_loss += batch_loss_val * batch_size\n",
    "                total_epoch_recon_loss += batch_recon_loss_val * batch_size\n",
    "                total_epoch_kl_loss += batch_kl_loss_val * batch_size\n",
    "                samples_processed_this_epoch += batch_size\n",
    "\n",
    "                if samples_processed_this_epoch > 0:\n",
    "                    progress_bar.set_postfix({\n",
    "                        'Loss': f\"{total_epoch_loss / samples_processed_this_epoch:.4f}\",\n",
    "                        'Recon': f\"{total_epoch_recon_loss / samples_processed_this_epoch:.4f}\",\n",
    "                        'KL': f\"{total_epoch_kl_loss / samples_processed_this_epoch:.4f}\"\n",
    "                    })\n",
    "\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "            if samples_processed_this_epoch > 0:\n",
    "                 avg_loss = total_epoch_loss / samples_processed_this_epoch\n",
    "                 avg_recon_loss = total_epoch_recon_loss / samples_processed_this_epoch\n",
    "                 avg_kl_loss = total_epoch_kl_loss / samples_processed_this_epoch\n",
    "\n",
    "                 print(f\"Epoch [{self.current_epoch}/{total_target_epochs}] Summary\"\n",
    "                       f\" | Avg Loss: {avg_loss:.4f}\"\n",
    "                       f\" | Recon: {avg_recon_loss:.4f}\"\n",
    "                       f\" | KL: {avg_kl_loss:.4f}\"\n",
    "                       f\" | Time: {epoch_duration:.2f}s\"\n",
    "                       f\" | LR: {self.optimizer.param_groups[0]['lr']:.1e}\")\n",
    "            else:\n",
    "                 print(f\"Epoch [{self.current_epoch}/{total_target_epochs}] | No samples processed successfully. Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "\n",
    "            self.save_checkpoint(epoch_completed=self.current_epoch)\n",
    "\n",
    "            is_last_epoch_overall = (self.current_epoch == total_target_epochs)\n",
    "            if save_every > 0 and (self.current_epoch % save_every == 0 or is_last_epoch_overall):\n",
    "                if not is_last_epoch_overall:\n",
    "                    epoch_save_path = f\"{os.path.splitext(self.model_save_path)[0]}_epoch_{self.current_epoch}.pth\"\n",
    "                    self.save_model(epoch_save_path)\n",
    "\n",
    "            # --- Periodic Image Generation --- ## <<<< LINE CORRECTED HERE >>>> ##\n",
    "            if self.generate_every_epochs > 0 and (self.current_epoch % self.generate_every_epochs == 0 or is_last_epoch_overall):\n",
    "                 print(f\"\\n--- Generating samples at end of Epoch {self.current_epoch} ---\")\n",
    "                 generated_samples = self.generate_images(num_images=NUM_IMAGES_TO_GENERATE)\n",
    "                 if generated_samples is not None:\n",
    "                     visualize_and_save_samples(generated_samples, self.current_epoch, self.results_save_dir, prefix=\"intermediate\")\n",
    "                 self.model.train() # Switch back to train mode after generation\n",
    "\n",
    "\n",
    "        total_training_time = time.time() - start_time_total\n",
    "        print(f\"\\n--- Training Loop Finished ---\")\n",
    "        print(f\"Completed epochs {initial_epoch_completed + 1} through {self.current_epoch}\")\n",
    "        print(f\"Total Time for this run: {total_training_time // 60:.0f}m {total_training_time % 60:.0f}s\")\n",
    "\n",
    "        if self.current_epoch == total_target_epochs:\n",
    "             print(\"\\nSaving final model state (target epoch reached)...\")\n",
    "             self.save_model(self.model_save_path)\n",
    "        else:\n",
    "             print(f\"\\nTarget epoch {total_target_epochs} not reached (stopped at {self.current_epoch}). Final model not saved to '{self.model_save_path}'. Use the latest checkpoint.\")\n",
    "\n",
    "\n",
    "    def generate_images(self, num_images=16):\n",
    "        \"\"\"Generates images from random noise using the decoder.\"\"\"\n",
    "        if num_images <= 0:\n",
    "            print(\"Number of images to generate must be positive.\")\n",
    "            return None\n",
    "        print(f\"Generating {num_images} images...\")\n",
    "        self.model.eval()\n",
    "        generated_images = None\n",
    "        try:\n",
    "            latent_dim = self.model.latent_dim\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(num_images, latent_dim).to(self.device)\n",
    "                generated_images = self.model.decode(z)\n",
    "            print(\"Image generation complete.\")\n",
    "            return generated_images.cpu()\n",
    "        except AttributeError as e:\n",
    "             print(f\"Error generating images: Model might be missing 'latent_dim' attribute or 'decode' method. {e}\")\n",
    "             return None\n",
    "        except Exception as e:\n",
    "             print(f\"An unexpected error occurred during image generation: {e}\")\n",
    "             traceback.print_exc()\n",
    "             return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Step 5.1: Run Preprocessor ---\n",
    "    print(\"\\n--- Running Preprocessor ---\")\n",
    "    train_output_dir = os.path.join(PROCESSED_DATA_ROOT, 'train')\n",
    "    if os.path.exists(train_output_dir) and len(os.listdir(train_output_dir)) > 0:\n",
    "         print(f\"Processed data found at '{train_output_dir}'. Skipping preprocessing.\")\n",
    "    else:\n",
    "        print(f\"Processed data not found or empty at '{train_output_dir}'. Running preprocessing...\")\n",
    "        try:\n",
    "            preprocessor = AnimeDatasetPreprocessor(\n",
    "                input_dir=RAW_DATASET_DIR,\n",
    "                output_dir=PROCESSED_DATA_ROOT,\n",
    "                sample_size=SAMPLE_SIZE_PREPROCESSOR,\n",
    "                target_size=VIT_INPUT_SIZE\n",
    "            )\n",
    "            preprocessor.prepare_dataset()\n",
    "            if not os.path.exists(train_output_dir) or len(os.listdir(train_output_dir)) == 0:\n",
    "                 raise RuntimeError(\"Preprocessing finished, but output training directory is still empty or missing!\")\n",
    "            print(\"Preprocessing completed successfully.\")\n",
    "        except NameError:\n",
    "            print(\"ERROR: AnimeDatasetPreprocessor class not found. Please define or import it.\")\n",
    "            sys.exit(\"Preprocessing definition failed.\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"\\nCritical Error during preprocessing setup: {e}\")\n",
    "            print(f\"Please ensure the RAW_DATASET_DIR ('{RAW_DATASET_DIR}') exists.\")\n",
    "            sys.exit(\"Preprocessing failed. Cannot continue.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred during preprocessing: {e}\")\n",
    "            traceback.print_exc()\n",
    "            sys.exit(\"Preprocessing failed. Cannot continue.\")\n",
    "    print(\"--- Preprocessor Step Done ---\")\n",
    "\n",
    "\n",
    "    # --- Step 5.2: Create DataLoader ---\n",
    "    print(\"\\n--- Setting up DataLoader ---\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    try:\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        train_dataset = ImageDatasetNoLabels(\n",
    "            root_dir=train_output_dir,\n",
    "            transform=data_transform\n",
    "        )\n",
    "        if len(train_dataset) == 0:\n",
    "             raise RuntimeError(f\"Dataset created but contains 0 images. Check path: {train_output_dir}\")\n",
    "\n",
    "        num_loader_workers = 2 if device.type == 'cuda' else 0\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "            num_workers=num_loader_workers, pin_memory=(num_loader_workers > 0),\n",
    "            persistent_workers=(num_loader_workers > 0), drop_last=True\n",
    "        )\n",
    "        print(f\"DataLoader created for {len(train_dataset)} samples.\")\n",
    "        print(f\"Batch size: {BATCH_SIZE}, Batches/epoch: {len(train_loader)}, Workers: {num_loader_workers}\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"ERROR: ImageDatasetNoLabels class not found. Please define or import it.\")\n",
    "        sys.exit(\"DataLoader setup failed.\")\n",
    "    except FileNotFoundError as e:\n",
    "         print(f\"Error: Failed to create Dataset - Path not found: {e}\")\n",
    "         sys.exit(\"DataLoader setup failed (FileNotFound).\")\n",
    "    except RuntimeError as e:\n",
    "         print(f\"Error: Failed to create Dataset or DataLoader: {e}\")\n",
    "         sys.exit(\"DataLoader setup failed (RuntimeError).\")\n",
    "    except Exception as e:\n",
    "         print(f\"An unexpected error occurred during DataLoader setup: {e}\")\n",
    "         traceback.print_exc()\n",
    "         sys.exit(\"DataLoader setup failed (Unexpected).\")\n",
    "    print(\"--- DataLoader Setup Done ---\")\n",
    "\n",
    "\n",
    "    # --- Step 5.3: Initialize Model and Trainer (with Resume Logic) ---\n",
    "    print(\"\\n--- Initializing Model and Trainer ---\")\n",
    "    try:\n",
    "        checkpoint_dir = os.path.join(DRIVE_SAVE_DIR, 'checkpoints')\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'anime_vae_vit_checkpoint.pth')\n",
    "        final_model_save_path = os.path.join(DRIVE_SAVE_DIR, f'anime_vae_vit_final_{NUM_EPOCHS}epochs.pth')\n",
    "        print(f\"Final model path: {final_model_save_path}\")\n",
    "        print(f\"Checkpoint path: {checkpoint_path}\")\n",
    "\n",
    "        vae_model = VisionTransformerVAE(\n",
    "            latent_dim=LATENT_DIM, vit_input_size=VIT_INPUT_SIZE\n",
    "        )\n",
    "        print(f\"{type(vae_model).__name__} structure initialized.\")\n",
    "\n",
    "        start_epoch = 0\n",
    "        optimizer_state_dict = None\n",
    "        model_loaded_from_checkpoint = False\n",
    "\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            print(f\"\\nCheckpoint found: '{checkpoint_path}'. Loading...\")\n",
    "            try:\n",
    "                checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "                print(\"Checkpoint dictionary loaded.\")\n",
    "\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    vae_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    print(\" -> Model weights loaded.\")\n",
    "                    model_loaded_from_checkpoint = True\n",
    "                else: print(\" -> WARNING: 'model_state_dict' missing.\")\n",
    "\n",
    "                if 'optimizer_state_dict' in checkpoint:\n",
    "                    optimizer_state_dict = checkpoint['optimizer_state_dict']\n",
    "                    print(\" -> Optimizer state dict found.\")\n",
    "                else: print(\" -> WARNING: 'optimizer_state_dict' missing.\")\n",
    "\n",
    "                start_epoch = checkpoint.get('epoch', 0)\n",
    "                print(f\" -> Last completed epoch: {start_epoch}.\")\n",
    "                if 'epoch' not in checkpoint: print(\"    (Warning: 'epoch' key missing, using 0)\")\n",
    "\n",
    "                loaded_lr = checkpoint.get('learning_rate')\n",
    "                if loaded_lr is not None: print(f\" -> Checkpoint LR: {loaded_lr}\")\n",
    "                loaded_beta = checkpoint.get('beta')\n",
    "                if loaded_beta is not None: print(f\" -> Checkpoint Beta: {loaded_beta}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nERROR loading checkpoint: {e}. Starting fresh.\")\n",
    "                traceback.print_exc()\n",
    "                start_epoch = 0; optimizer_state_dict = None; model_loaded_from_checkpoint = False\n",
    "        else:\n",
    "            print(f\"\\nNo checkpoint found. Starting fresh.\")\n",
    "            start_epoch = 0; optimizer_state_dict = None; model_loaded_from_checkpoint = False\n",
    "\n",
    "        vae_model.to(device)\n",
    "        print(f\"\\nModel moved to: {device}\")\n",
    "\n",
    "        actual_start_epoch_for_loop = start_epoch + 1\n",
    "        epochs_remaining_to_run = max(0, NUM_EPOCHS - start_epoch)\n",
    "        print(\"-\" * 40)\n",
    "        print(\" Pre-Trainer Init Summary \".center(40, \"-\"))\n",
    "        print(f\"  Model Loaded: {'Yes' if model_loaded_from_checkpoint else 'No'}\")\n",
    "        print(f\"  Optimizer State Found: {'Yes' if optimizer_state_dict is not None else 'No'}\")\n",
    "        print(f\"  Last Completed Epoch: {start_epoch}\")\n",
    "        print(f\"  Next Epoch to Run: {actual_start_epoch_for_loop}\")\n",
    "        print(f\"  Epochs Remaining: {epochs_remaining_to_run}\")\n",
    "        print(f\"  LR for Trainer: {LEARNING_RATE}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        trainer = AnimeGANTrainer(\n",
    "            model=vae_model, learning_rate=LEARNING_RATE, beta=BETA, device=device,\n",
    "            model_save_path=final_model_save_path, checkpoint_path=checkpoint_path,\n",
    "            generate_every_epochs=GENERATE_EVERY_EPOCHS, start_epoch=start_epoch,\n",
    "            optimizer_state=optimizer_state_dict\n",
    "        )\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"ERROR: Class definition missing (e.g., VisionTransformerVAE). Details: {e}\")\n",
    "        sys.exit(\"Initialization failed - Missing Class.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCRITICAL ERROR during Model/Trainer Initialization: {e}\")\n",
    "        traceback.print_exc()\n",
    "        sys.exit(\"Initialization failed.\")\n",
    "    print(\"--- Model and Trainer Initialized ---\")\n",
    "\n",
    "\n",
    "    # --- Step 5.4: Train the Model ---\n",
    "    print(\"\\n--- Verifying DataLoader for Training ---\")\n",
    "    try:\n",
    "        num_batches = len(train_loader)\n",
    "        if num_batches == 0: raise ValueError(\"train_loader is empty.\")\n",
    "        print(f\"DataLoader OK: {len(train_dataset)} samples, {num_batches} batches.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying DataLoader: {e}\")\n",
    "        sys.exit(\"ERROR: DataLoader verification failed.\")\n",
    "\n",
    "    print(\"\\n--- Preparing for Model Training ---\")\n",
    "    try:\n",
    "        epochs_already_done = trainer.current_epoch\n",
    "        epochs_to_run = NUM_EPOCHS - epochs_already_done\n",
    "        generate_final_images = False # Default\n",
    "\n",
    "        if epochs_to_run <= 0:\n",
    "            print(f\"\\nModel already trained for {epochs_already_done} epochs (Target: {NUM_EPOCHS}). Skipping training.\")\n",
    "            generate_final_images = True\n",
    "        else:\n",
    "             print(f\"Starting training: Epoch {epochs_already_done + 1} -> {NUM_EPOCHS} ({epochs_to_run} epochs)\")\n",
    "             trainer.train(\n",
    "                 train_loader, epochs_to_run=epochs_to_run,\n",
    "                 total_target_epochs=NUM_EPOCHS, save_every=SAVE_EVERY_EPOCHS\n",
    "             )\n",
    "             print(f\"\\nTraining run finished. Last completed epoch: {trainer.current_epoch}.\")\n",
    "             generate_final_images = True\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Training Interrupted (KeyboardInterrupt) ---\")\n",
    "        print(\"Attempting checkpoint save...\")\n",
    "        trainer.save_checkpoint(epoch_completed=trainer.current_epoch)\n",
    "        print(f\"Training stopped. Last completed epoch: {trainer.current_epoch}.\")\n",
    "        generate_final_images = True # Allow generation after interrupt\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An unexpected error occurred during training: {e} ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Attempting checkpoint save...\")\n",
    "        trainer.save_checkpoint(epoch_completed=trainer.current_epoch)\n",
    "        print(f\"Training halted. Last completed epoch: {trainer.current_epoch}.\")\n",
    "        generate_final_images = False # Skip generation on error\n",
    "        sys.exit(\"Training halted due to error.\")\n",
    "    print(\"--- Model Training Step Finished ---\")\n",
    "\n",
    "\n",
    "    # --- Step 5.5: Generate Final Images ---\n",
    "    if generate_final_images:\n",
    "        print(\"\\n--- Generating Final Sample Images ---\")\n",
    "        try:\n",
    "            final_generated_samples = trainer.generate_images(num_images=NUM_IMAGES_TO_GENERATE)\n",
    "            if final_generated_samples is not None:\n",
    "                 print(f\"Generated final samples tensor shape: {final_generated_samples.shape}\")\n",
    "            else: print(\"Image generation returned None.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during final image generation: {e}\")\n",
    "            final_generated_samples = None\n",
    "        print(\"--- Final Image Generation Done ---\")\n",
    "    else:\n",
    "        print(\"\\n--- Skipping Final Image Generation ---\")\n",
    "        final_generated_samples = None\n",
    "\n",
    "\n",
    "    # --- Step 5.6: Visualize and Save Final Generated Images ---\n",
    "    if final_generated_samples is not None:\n",
    "        print(\"\\n--- Visualizing and Saving Final Generated Images ---\")\n",
    "        try:\n",
    "            last_completed_epoch = trainer.current_epoch\n",
    "            visualize_and_save_samples(\n",
    "                final_generated_samples, epoch=last_completed_epoch,\n",
    "                save_dir=DRIVE_SAVE_DIR, prefix=f\"final_generated_epoch{last_completed_epoch}\"\n",
    "            )\n",
    "            print(f\"Saved final generated images for epoch {last_completed_epoch}.\")\n",
    "        except NameError:\n",
    "            print(\"ERROR: visualize_and_save_samples function not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during visualization/saving: {e}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping final visualization/saving.\")\n",
    "\n",
    "    print(\"\\n SCRIPT COMPLETE \".center(80, \"=\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
